# File to be imported in all scripts
    # Saves paths used in the project
    # Saves common functions and variables used in the project

import os

# PATHS
PATH_DATASETS  = os.path.dirname(os.path.abspath(__file__)) + '/../datasets'

PATH_SPIDER_FULL = PATH_DATASETS + "/spider_data"

PATH_MMLU_PROCESSED = PATH_DATASETS + "/mmlu_processed_150.json"
PATH_SPIDER_DEV_PROCESSED = PATH_DATASETS + "/spider_dev_processed.json"
PATH_SPIDER_TEST_PROCESSED = PATH_DATASETS + "/spider_test_processed.json"

CHECKPOINTS_DIR = os.path.dirname(os.path.abspath(__file__)) + '/../checkpoints'
if not os.path.exists(CHECKPOINTS_DIR):
    os.makedirs(CHECKPOINTS_DIR)
CHECKPOINT_BASELINE = CHECKPOINTS_DIR + "/baseline_checkpoint.json"
CHECKPOINT_LORAS = CHECKPOINTS_DIR + "/loras"

# COMMON VARIABLES AND FUNCTIONS

SEED = 42

# BASE_MODEL_ID = "Qwen/Qwen2.5-0.5B-Instruct"
BASE_MODEL_ID = "Qwen/Qwen2-7B-Instruct" # used model
BASE_MODEL_NAME = BASE_MODEL_ID.split("/")[-1] # used model name

EVALUATION_ENTRIES_SIZE = 150
from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM
from peft import prepare_model_for_kbit_training
import torch

def custom_load_model(model_path, quantization="4bit", goal="inference"):
    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    tokenizer.padding_side = "left"
    tokenizer.pad_token = tokenizer.pad_token or tokenizer.eos_token

    # Model loading configuration
    model_kwargs = {
        "device_map": "auto",
        # "attn_implementation": "flash_attention_2",
        "torch_dtype": torch.float16,
    }

    if quantization == "4bit":
        model_kwargs["quantization_config"] = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=False,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
        )

    # Load model
    model = AutoModelForCausalLM.from_pretrained(model_path, **model_kwargs)

    # Prepare for k-bit training if needed
    if quantization == "4bit" and goal == "training":
        model = prepare_model_for_kbit_training(model)

    return tokenizer, model

def spider_to_sql_full_prompt(entry):
    return (
        f"<|im_start|>system\n"
        f"You are an expert text-to-SQL assistant.<|im_end|>\n"
        f"<|im_start|>user\n"
        f"Given the following database schema:\n"
        f"{entry['db_schema']}\n"
        f"Generate the SQL query for the following question:\n"
        f"{entry['question']}<|im_end|>\n"
        f"<|im_start|>assistant\n"
        f"{entry['query']}<|im_end|>"
    )

def spider_to_sql_inference_prompt(entry):
    return (
        f"<|im_start|>system\n"
        f"You are an expert text-to-SQL assistant.<|im_end|>\n"
        f"<|im_start|>user\n"
        f"Given the following database schema:\n"
        f"{entry['db_schema']}\n"
        f"Generate the SQL query for the following question:\n"
        f"{entry['question']}<|im_end|>\n"
        f"<|im_start|>assistant\n"
    )

def clean_model_output(generated_text):
    # Remove everything after <im_end>
    output = generated_text.split('<im_end>')[0].strip()
    # If markdown code block is present, extract only what is inside it
    if "```sql" in output:
        output = output.split("```sql")[1].split("```")[0].strip()
    elif "```" in output:
        output = output.split("```")[1].split("```")[0].strip()
    return output

# returns the output generated by the model for a given prompt
def execute_prompt(model, tokenizer, prompt):
    input_ids = tokenizer(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model.generate(
            **input_ids,
            max_new_tokens=512,
            pad_token_id=tokenizer.eos_token_id,
            do_sample=False, # deterministic generation
            temperature=None,
            top_p=None,
            top_k=None
        )
    generated_text = tokenizer.decode(outputs[0][len(input_ids.input_ids[0]):], skip_special_tokens=True).strip()
    return clean_model_output(generated_text)